name: Run recipe 
run-name: Run ${{ inputs.recipe_name }} from ${{ inputs.repo }} by @${{ github.actor }}

on:
  workflow_dispatch:
    inputs:
      aws_access_key_id:
        required: True
      aws_secret_access_key:
        required: True
      aws_session_token:
        required: True
      recipe_url:
        description: 'The https github url for the recipe feedstock'
        required: true
      recipe_ref:
        description: 'The tag or branch to target in your recipe repo'
        required: true
        default: 'main'
      feedstock_subdir:
        description: 'The subdir of the feedstock directory in the repo'
        required: true
        default: 'feedstock'
      recipe_name:
        required: True
        default: recipe
      bucket:
        description: 'This job runner leverages s3fs.S3FileSystem for your recipe cache and output. Choices currently are: "default"'
        required: true
        default: 'default'
      prune:
        description: 'Only run the first two time steps'
        required: true
        default: '0'
      parallelism:
        description: 'Number of task managers to spin up'
        required: true
        default: '1'

jobs:
  run-job:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: set up aws credentials for job runner user
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ inputs.aws_access_key_id }}
          aws-secret-access-key: ${{ inputs.aws_secret_access_key }}
          aws-session-token: ${{ inputs.aws_session_token }}

      - name: install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x ./kubectl
          sudo mv ./kubectl /usr/local/bin/kubectl

      - name: update kubeconfig with cluster
        run: |
          aws eks update-kubeconfig --name ${{ inputs.cluster_name }} --region ${{ inputs.aws_region }}

      - name: set up python 3.10
        uses: actions/setup-python@v3
        with:
          python-version: '3.10'
          
      - name: install deps
        run: |
          pip install requirements.txt

      - name: execute recipe on k8s cluster
        id: executejob
        continue-on-error: true
        run: |
          # NOTE: we can't use `2>&1 | tee execute.log` b/c it hangs forever
          # so if the command fails (for example b/c it doesn't have the right requirements)
          # then we wont' be able to see the errors until we run it without redirecting output
          pangeo-forge-runner \
            bake \
            --repo=${{ github.event.inputs.repo }} \
            --ref=${{ github.event.inputs.ref }} \
            -f .github/workflows/config.py > execute.log
            
          # export all the valuable information from the logs
          JOB_NAME=$(cat execute.log | grep -oP 'flinkdeployment\.flink\.apache\.org/\K[^ ]+' | head -n1)
          echo "JOB_NAME=$JOB_NAME" >> $GITHUB_ENV

          JOB_ID=$(cat execute.log | grep -oP 'Started Flink job as \K[^ ]+')
          echo "JOB_ID=$JOB_ID" >> $GITHUB_ENV

          FLINK_DASH=$(cat execute.log | grep -oP "You can run '\K[^']+(?=')")
          echo "FLINK_DASH=$FLINK_DASH" >> $GITHUB_ENV
        env:
          RECIPE_URL: ${{ inputs.recipe_url }}
          RECIPE_REF: ${{ inputs.recipe_ref }}
          RECIPE_NAME: ${{ inputs.recipe_name}
          FEEDSTOCK_SUBDIR: ${{ inputs.feedstock_subdir }}
          PRUNE_OPTION: ${{ inputs.prune }}
          PARALLELISM_OPTION: ${{ inputs.parallelism }}
          S3_BUCKET: ${{ inputs.bucket }}
          S3_DEFAULT_AWS_ACCESS_KEY_ID: ${{ secrets.S3_DEFAULT_AWS_ACCESS_KEY_ID }}
          S3_DEFAULT_AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DEFAULT_AWS_SECRET_ACCESS_KEY }}
